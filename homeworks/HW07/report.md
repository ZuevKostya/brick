# HW07 – Report

> Файл: `homeworks/HW07/report.md`  
> Важно: не меняйте названия разделов (заголовков). Заполняйте текстом и/или вставляйте результаты.

## 1. Datasets

Вы выбрали 3 датасета из 4 (перечислите):
S07-hw-dataset-01.csv, 
S07-hw-dataset-02.csv, 
S07-hw-dataset-03.csv
### 1.1 Dataset A

- Файл: `S07-hw-dataset-01.csv`
- Размер: (12000, 9)
- Признаки: числовые
- Пропуски: нет
- "Подлости" датасета: разные шкалы / шумовые признаки

### 1.2 Dataset B

- Файл: `S07-hw-dataset-02.csv`
- Размер: (8000, 4)
- Признаки: числовые
- Пропуски: нет
- "Подлости" датасета: нелинейная структура + выбросы + лишний шумовой признак.


### 1.3 Dataset C

- Файл: `S07-hw-dataset-03.csv`
- Размер: (15000, 5)
- Признаки: числовые
- Пропуски: нет
- "Подлости" датасета: кластеры разной плотности + фоновый шум

## 2. Protocol

Опишите ваш "честный" unsupervised-протокол.

- Препроцессинг: scaling StandardScaler, imputation mean, encoding get_dummies
- Поиск гиперпараметров:
  - для KMeans k=2-10 , для DBSCAN eps=0.5-1.5, min_samples=3-7 
  - чем руководствовались при выборе "лучшего": max silhouette
- Метрики: silhouette / Davies-Bouldin / Calinski-Harabasz (и как считали для DBSCAN при наличии шума)исключали шум -1
- Визуализация: PCA(2D) (и t-SNE, если делали – с какими параметрами) PCA n_components=2, random_state=42; t-SNE perplexity=30

## 3. Models

Перечислите, какие модели сравнивали **на каждом датасете**, и какие параметры подбирали.

Минимум (для каждого датасета):

- KMeans k на 1 = 2, k на 2 = 2, k на 3 = 3 , random_state=42, n_init=10
- Один из:
  - DBSCAN eps=0.5, min_samples=5



## 4. Results

Для каждого датасета – краткая сводка результатов.

### 4.1 Dataset A

- Лучший метод и параметры: KMeans k=2
Метрики (silhouette / DB / CH): silhouette=0.52 / DB=0.68 / CH=11786
Если был DBSCAN: доля шума и комментарий: 0.05, мало шума
Коротко: почему это решение выглядит разумным именно для этого датасета: разные шкалы, scaling помог, KMeans справился после

### 4.2 Dataset B

Лучший метод и параметры: DBSCAN eps=0.5, min_samples=5
Метрики (silhouette / DB / CH): silhouette=0.5 / DB=0.7 / CH=150
Если был DBSCAN: доля шума и комментарий: 0.1, выделил выбросы
Коротко: почему это решение выглядит разумным именно для этого датасета: нелинейная структура, DBSCAN лучше KMeans

### 4.3 Dataset C

Лучший метод и параметры: DBSCAN eps=0.8, min_samples=4
Метрики (silhouette / DB / CH): silhouette=0.4 / DB=0.9 / CH=100
Если был DBSCAN: доля шума и комментарий: 0.15, шум от разной плотности
Коротко: почему это решение выглядит разумным именно для этого датасета: разная плотность, DBSCAN справился

## 5. Analysis

### 5.1 Сравнение алгоритмов (важные наблюдения)

- Где KMeans "ломается" и почему?
- Где DBSCAN/иерархическая кластеризация выигрывают и почему?
- Что сильнее всего влияло на результат (масштабирование, выбросы, плотность, пропуски, категориальные признаки)?

### 5.2 Устойчивость (обязательно для одного датасета)

- Какую проверку устойчивости делали (5 запусков KMeans по разным seed или иной подход): 5 seed для KMeans на A
- Что получилось (в 3-6 строк): Silhouette: [0.45, 0.44, 0.46, 0.45, 0.44]
- Вывод: устойчиво/неустойчиво и почему вы так считаете: Устойчиво, STD=0.008 низкий

### 5.3 Интерпретация кластеров

- Как вы интерпретировали кластеры:
  - профили признаков (средние/медианы) **или**
  - любая другая логичная интерпретация.Средние по кластерам показывают различия в шкалах
- 3-6 строк выводов. Кластер 0: высокие значения, кластер 1: низкие. Различия по плотности. Шум - выбросы.

## 6. Conclusion

4-8 коротких тезисов: чему научились про кластеризацию, метрики и корректный протокол unsupervised-эксперимента.
1)Нужен предпроцессинг.
2)Метрики и визуализация важны.
3)DBSCAN хорошо работает с шумом.
4)Устойчивость можно проверять изменяя seed.
5)Протокол позволяет эксперименту оставатся честным.